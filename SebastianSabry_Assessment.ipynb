{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da6991d-4b82-4f07-9660-a94d489ab3a7",
   "metadata": {},
   "source": [
    "## Installation Reqs (Linux Ubuntu)\n",
    "1. <code> python3 -m pip install playwright\n",
    "playwright install </code> \n",
    "2. <code> pip install datasets <code>\n",
    "3. <code> pip install transformers <code>\n",
    "4. <code> wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
    "sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda-repo-ubuntu2204-12-4-local_12.4.0-550.54.14-1_amd64.deb\n",
    "sudo dpkg -i cuda-repo-ubuntu2204-12-4-local_12.4.0-550.54.14-1_amd64.deb\n",
    "sudo cp /var/cuda-repo-ubuntu2204-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "sudo apt-get update\n",
    "sudo apt-get -y install cuda-toolkit-12-4 <code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd670e-a965-4566-b161-afc830bf89ce",
   "metadata": {},
   "source": [
    "## Webscraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccba65b2-8fe7-4ae8-8db9-779559daa472",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WEB SCRAPING CELL: \n",
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "\n",
    "async def process_locator(locator):\n",
    "    count = await locator.count()\n",
    "    if count > 1: #We have many elements and must resolve each inner text\n",
    "        texts = \"\"\n",
    "        for i in range(count):\n",
    "            element = locator.nth(i)\n",
    "            if await element.is_visible():\n",
    "                inner_text = await element.inner_text()\n",
    "                texts = texts + \",\" + inner_text\n",
    "                return texts\n",
    "    else:\n",
    "        if await locator.is_visible():\n",
    "            return await locator.inner_text()\n",
    "        else:\n",
    "            return \"NA\"\n",
    "    \n",
    "\n",
    "async def main():\n",
    "   async with async_playwright() as pw:\n",
    "       browser = await pw.chromium.launch(\n",
    "           ##We'll employ the use of chromium for this webscraper\n",
    "           ##Using a proxy creates HTTP errors.\n",
    "          headless=False\n",
    "      )\n",
    "\n",
    "       #Beginning page: \n",
    "       page = await browser.new_page()\n",
    "       await page.goto('https://world.openfoodfacts.org/')\n",
    "       await page.wait_for_timeout(5000)\n",
    "       result = []\n",
    "       food_urls = []\n",
    "       food_list = await page.query_selector_all('.list_product_a')\n",
    "       for food in food_list:\n",
    "           food_urls.append(await food.get_attribute('href'))\n",
    "           \n",
    "       for food_url in food_urls:\n",
    "            food_info = {}\n",
    "            await page.goto(food_url)\n",
    "            #Title: \n",
    "            title = page.locator(\".title-1\")\n",
    "            food_info['title'] = await process_locator(title)\n",
    "            #Common Name:\n",
    "            common_name = page.locator(\"#field_generic_name_value\")\n",
    "            food_info['common_name'] = await process_locator(common_name)\n",
    "            #Quantity:\n",
    "            quantity = page.locator(\"#field_quantity_value\")\n",
    "            food_info['quantity'] = await process_locator(quantity)\n",
    "            #Packaging: \n",
    "            packaging = page.locator(\"#field_packaging_value\")\n",
    "            food_info['packaging'] = await process_locator(packaging)\n",
    "            #Brands:\n",
    "            brand = page.locator(\"#field_brands_value\")\n",
    "            food_info['brand'] = await process_locator(brand)\n",
    "            #Categories:\n",
    "            categories = page.locator(\"#field_categories_value\")\n",
    "            food_info['categories'] = await process_locator(categories)\n",
    "            #Certifications:\n",
    "            certifications = page.locator(\"#field_labels_value\")\n",
    "            food_info['certifications'] = await process_locator(certifications)\n",
    "            #Origin:\n",
    "            origin = page.locator(\"#field_origin_value\")\n",
    "            food_info['origin'] = await process_locator(origin)\n",
    "            #origin of ingredients:\n",
    "            origin_of_ingredients = page.locator(\"#field_origins_value\")\n",
    "            food_info['origin_of_ingredients'] = await process_locator(origin_of_ingredients)\n",
    "            #Places of manufacturing:\n",
    "            places_manufactured = page.locator(\"#field_manufacturing_places_value\")\n",
    "            food_info['places_manufactured'] = await process_locator(places_manufactured)\n",
    "            #Stores:\n",
    "            stores = page.locator(\"#field_stores_value\")\n",
    "            food_info['stores'] = await process_locator(stores)\n",
    "            #Countries where Sold:\n",
    "            countries_sold = page.locator(\"#field_countries_value\")\n",
    "            food_info['countries_sold'] = await process_locator(countries_sold)\n",
    "           \n",
    "            #HEALTH SECTION\n",
    "            #Notice, because of the increasing complexity of the DOM elements in this area the CSS selectors don't follow a similarly nice pattern\n",
    "            #Ingredients: \n",
    "            ingredients = page.locator(\"#panel_ingredients_content .panel_text\")\n",
    "            food_info['ingredients'] = await process_locator(ingredients)\n",
    "            #NOVA score:\n",
    "            nova_score = page.locator(\"ul#panel_nova li.accordion-navigation h4\")\n",
    "            food_info['nova_score'] = await process_locator(nova_score)\n",
    "            # #Palm Status:\n",
    "            # palm_status = page.locator(\".accordion-navigation active .content panel_content active .panel_text\")\n",
    "            # food_info['palm_status'] = await process_locator(palm_status)\n",
    "            # #Vegan Status:\n",
    "            # vegan_status = page.locator(\"#panel_ingredients_analysis_en-vegan_content .panel_text\")\n",
    "            # food_info['vegan_status'] = await process_locator(vegan_status)\n",
    "            # #Vegetarian Status:\n",
    "            # vegetarian_status = page.locator(\"#panel_ingredients_analysis_en-vegetarian_content .panel_text\")\n",
    "            # food_info['vegetarian_status'] = await process_locator(vegetarian_status)\n",
    "            #Nutrition grade:\n",
    "            nutrition_grade = page.locator(\".accordion-navigation .grade_a_title\")\n",
    "            food_info['nutrition_grade'] = await process_locator(nutrition_grade)\n",
    "\n",
    "            # #NUTITRION FACTS\n",
    "            # #\n",
    "            # table_rows = await page.query_selector_all(\"#panel_nutrition_facts_table_content\")\n",
    "            # nutrition_facts = {}\n",
    "            # for row in table_rows:\n",
    "            #     columns = await row.query_selector_all('td')\n",
    "            #     name = await process_locator(columns[0])\n",
    "            #     value_per_100g = await process_locator(columns[1])\n",
    "            #     nutrition_facts[name] = {\n",
    "            #         \"100g/100ml\": value_per_100g\n",
    "            #     }\n",
    "                    \n",
    "                    \n",
    "            # food_info['nutrition_table'] = nutrition_facts\n",
    "            result.append(food_info)\n",
    "            \n",
    "\n",
    "\n",
    "       \n",
    "       \n",
    "\n",
    "       \n",
    "           \n",
    "           \n",
    "           \n",
    "       await browser.close()\n",
    "       return result\n",
    "if __name__ == '__main__':\n",
    "   result = await main()\n",
    "\n",
    "#Problems & Changes:\n",
    "#\n",
    "\n",
    "#CITATIONs: \n",
    "#Code cited from OxyLabs: https://github.com/oxylabs/playwright-web-scraping?tab=readme-ov-file\n",
    "#,https://playwright.dev/python/docs/locators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435918ad-288a-4d30-84c6-e1a8e6d5eced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"Huile d'olive vierge extra - Domaine de Bournissac - 500\\xa0ml\",\n",
       " 'common_name': \"Huile d'olive\",\n",
       " 'quantity': '500 ml',\n",
       " 'packaging': 'NA',\n",
       " 'brand': 'Domaine de Bournissac',\n",
       " 'categories': 'Plant-based foods and beverages, Plant-based foods, Fats, Vegetable fats, Olive tree products, Vegetable oils, Olive oils, Extra-virgin olive oils, Virgin olive oils',\n",
       " 'certifications': 'Organic, EU Organic, FR-BIO-10\\n',\n",
       " 'origin': 'NA',\n",
       " 'origin_of_ingredients': 'NA',\n",
       " 'places_manufactured': 'NA',\n",
       " 'stores': 'NA',\n",
       " 'countries_sold': 'France',\n",
       " 'ingredients': 'NA',\n",
       " 'nova_score': 'Processed culinary ingredients',\n",
       " 'nutrition_grade': 'NA'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb22bb-8f09-4063-a5d4-c19d84db4a06",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e20777-da2e-44b2-bbf4-3b92bbdcb6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastiancsabry/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING CELL\n",
    "from datasets import DatasetDict, Dataset, Features\n",
    "##DATA FORMAT:\n",
    "# Context: 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'\n",
    "# Question: 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'\n",
    "# Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
    "\n",
    "#Defining a function to create the context, in the context part of a QA data format\n",
    "def create_context(food_data):\n",
    "    context = (\n",
    "        f\"{food_data['title']} is commonly known as {food_data['common_name']}. \"\n",
    "        f\"The ingredients are {food_data['ingredients']}. \"\n",
    "        f\"The packaging includes {food_data['packaging']}. \"\n",
    "        f\"The brand is {food_data['brand']}. \"\n",
    "        f\"It falls under the categories {food_data['categories']}. \"\n",
    "        f\"It has certifications like {food_data['certifications']}. \"\n",
    "        f\"It originates from {food_data['origin']} and the origin of ingredients is {food_data['origin_of_ingredients']}. \"\n",
    "        f\"It is manufactured in {food_data['places_manufactured']}. \"\n",
    "        f\"It is sold in countries like {food_data['countries_sold']}. \"\n",
    "        f\"The nutrition grade is {food_data['nutrition_grade']}. \"\n",
    "        f\"The NOVA score is {food_data['nova_score']}. \"\n",
    "        f\"It can be found in stores such as {food_data['stores']}. \"\n",
    "    )\n",
    "    return context\n",
    "\n",
    "\n",
    "\n",
    "def create_question_answer(food_data, context):\n",
    "    qa_pairs = []\n",
    "\n",
    "    #Generate question about common name \n",
    "    question = \"What is the common name of \" + food_data['title'] + \"?\"\n",
    "    answer = {'text': [food_data['common_name']], 'answer_start':[context.index(food_data['common_name'])]}\n",
    "    qa_pairs.append({'question':question, 'answer':answer})\n",
    "    #Generate question about ingredients\n",
    "    question = \"What are some ingredients in \" + food_data['title'] + \"?\"\n",
    "    answer = {'text':[food_data['ingredients']], 'answer_start':[context.index(food_data['ingredients'])]}\n",
    "    qa_pairs.append({'question':question, 'answer':answer})\n",
    "    #Generate question \n",
    "    question = \"What is the packaging of \" + food_data['title'] + \"?\"\n",
    "    answer = {'text':[food_data['packaging']], 'answer_start':[context.index(food_data['packaging'])]}\n",
    "    qa_pairs.append({'question':question, 'answer':answer})\n",
    "    #Generate Question\n",
    "    question = \"What is the brand of \" + food_data['title'] + \"?\"\n",
    "    answer = {'text':[food_data['brand']], 'answer_start':[context.index(food_data['brand'])]}\n",
    "    qa_pairs.append({'question':question, 'answer':answer})\n",
    "    #Generate Question\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "def create_qac_dataset(food_data_list):\n",
    "    qac_dataset = []\n",
    "    for food_data in food_data_list:\n",
    "        context = create_context(food_data)\n",
    "        qa_pairs = create_question_answer(food_data, context)\n",
    "        for qa in qa_pairs:\n",
    "            current_dict = {\"context\": context, \"question\": qa['question'], \"answer\": qa['answer']}\n",
    "            qac_dataset.append(current_dict)\n",
    "    return qac_dataset\n",
    "        \n",
    "   #As of the current function, it is a deterministic split, this is done for debugging purposes\n",
    "def split_dataset(dataset, split_ratio=0.8):\n",
    "    split_index = int(len(dataset) * split_ratio)\n",
    "    \n",
    "    training_data = dataset[:split_index]\n",
    "    validation_data = dataset[split_index:]\n",
    "    \n",
    "    return {\"train\": training_data, \"validation\": validation_data}\n",
    "\n",
    "\n",
    "\n",
    "#This is the format found on the Hugging Face tutorial, following this contruction for simplicity\n",
    "def convert_to_dataset_dict(training_set, validation_set):\n",
    "    features = Features({\n",
    "        \"id\": \"string\",\n",
    "        \"title\": \"string\",\n",
    "        \"context\": \"string\",\n",
    "        \"question\": \"string\",\n",
    "        \"answers\": \"string\",\n",
    "    })\n",
    "\n",
    "    # Create Dataset objects\n",
    "    train_dataset = Dataset.from_pandas(training_set)\n",
    "    validation_dataset = Dataset.from_pandas(validation_set)\n",
    "\n",
    "    # Create DatasetDict\n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": validation_dataset\n",
    "    })\n",
    "\n",
    "    return dataset_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad1e100b-ab2b-461d-b9f2-f21ef9fd2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our question, answer, context dictionary\n",
    "qac_result = create_qac_dataset(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6bc2714-08a6-47c4-9cef-fc2d44faccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The documentation on Hugging face suggests that this is the best format\n",
    "idx = 0\n",
    "for qa in qac_result:\n",
    "    qa[\"id\"] = idx + 1  # Adding 1 to start id from 1\n",
    "    qa[\"title\"] = f\"Title {idx + 1}\"  # Assuming title follows a pattern, adjust as needed\n",
    "\n",
    "    # Move id and title to the beginning of the dictionary\n",
    "    qa.update({\"id\": qa[\"id\"], \"title\": qa[\"title\"]})\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52bc885d-91f4-4d19-8666-89dcddae786e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': \"Eau de Source - Cristaline - 1,5\\xa0L is commonly known as Spring water. The ingredients are water. The packaging includes Aluminium-can, HdpeFilm-packet, PpFilm-wrapper, Ldpe-film. The brand is Cristaline. It falls under the categories Beverages, Waters, Spring waters. It has certifications like Triman\\n. It originates from Embouteillée à 24610 Saint-Martin de Gurson France and the origin of ingredients is France, fr:Saint-Martin de Gurson. It is manufactured in Saint-Martin de Gurson, France, 24610. It is sold in countries like Belgium, Côte d'Ivoire, France, Germany, Guadeloupe, Italy, Luxembourg, Mali, Martinique, New Caledonia, Switzerland, United Kingdom. The nutrition grade is Very good nutritional quality. The NOVA score is Unprocessed or minimally processed foods. It can be found in stores such as Carrefour, Leclerc, Auchan, Intermarché, Super U, E.Leclerc. \",\n",
       " 'question': 'What is the common name of Eau de Source - Cristaline - 1,5\\xa0L?',\n",
       " 'answer': {'text': ['Spring water'], 'answer_start': [56]},\n",
       " 'id': 1,\n",
       " 'title': 'Title 1'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qac_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "992ce3d0-9b83-494d-b858-4ea6409fc532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eau de Source - Cristaline - 1,5 L is commonly...</td>\n",
       "      <td>What is the common name of Eau de Source - Cri...</td>\n",
       "      <td>{'text': ['Spring water'], 'answer_start': [56]}</td>\n",
       "      <td>1</td>\n",
       "      <td>Title 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eau de Source - Cristaline - 1,5 L is commonly...</td>\n",
       "      <td>What are some ingredients in Eau de Source - C...</td>\n",
       "      <td>{'text': ['water'], 'answer_start': [63]}</td>\n",
       "      <td>2</td>\n",
       "      <td>Title 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eau de Source - Cristaline - 1,5 L is commonly...</td>\n",
       "      <td>What is the packaging of Eau de Source - Crist...</td>\n",
       "      <td>{'text': ['Aluminium-can, HdpeFilm-packet, PpF...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eau de Source - Cristaline - 1,5 L is commonly...</td>\n",
       "      <td>What is the brand of Eau de Source - Cristalin...</td>\n",
       "      <td>{'text': ['Cristaline'], 'answer_start': [16]}</td>\n",
       "      <td>4</td>\n",
       "      <td>Title 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prince Chocolat biscuits au blé complet - Lu -...</td>\n",
       "      <td>What is the common name of Prince Chocolat bis...</td>\n",
       "      <td>{'text': ['BISCUITS FOURRÉS (35%) PARFUM CHOCO...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>Levure de bière - Gerblé - 150 g is commonly k...</td>\n",
       "      <td>What is the brand of Levure de bière - Gerblé ...</td>\n",
       "      <td>{'text': ['Gerblé'], 'answer_start': [18]}</td>\n",
       "      <td>316</td>\n",
       "      <td>Title 316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Granola - LU - 200 g e is commonly known as Bi...</td>\n",
       "      <td>What is the common name of Granola - LU - 200 ...</td>\n",
       "      <td>{'text': ['Biscuits sablés nappés de chocolat ...</td>\n",
       "      <td>317</td>\n",
       "      <td>Title 317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Granola - LU - 200 g e is commonly known as Bi...</td>\n",
       "      <td>What are some ingredients in Granola - LU - 20...</td>\n",
       "      <td>{'text': [',wheat flour 48%, milk chocolate 27...</td>\n",
       "      <td>318</td>\n",
       "      <td>Title 318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Granola - LU - 200 g e is commonly known as Bi...</td>\n",
       "      <td>What is the packaging of Granola - LU - 200 g e?</td>\n",
       "      <td>{'text': ['fr:sachet plastique, fr:étui carton...</td>\n",
       "      <td>319</td>\n",
       "      <td>Title 319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Granola - LU - 200 g e is commonly known as Bi...</td>\n",
       "      <td>What is the brand of Granola - LU - 200 g e?</td>\n",
       "      <td>{'text': ['LU'], 'answer_start': [10]}</td>\n",
       "      <td>320</td>\n",
       "      <td>Title 320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context  \\\n",
       "0    Eau de Source - Cristaline - 1,5 L is commonly...   \n",
       "1    Eau de Source - Cristaline - 1,5 L is commonly...   \n",
       "2    Eau de Source - Cristaline - 1,5 L is commonly...   \n",
       "3    Eau de Source - Cristaline - 1,5 L is commonly...   \n",
       "4    Prince Chocolat biscuits au blé complet - Lu -...   \n",
       "..                                                 ...   \n",
       "315  Levure de bière - Gerblé - 150 g is commonly k...   \n",
       "316  Granola - LU - 200 g e is commonly known as Bi...   \n",
       "317  Granola - LU - 200 g e is commonly known as Bi...   \n",
       "318  Granola - LU - 200 g e is commonly known as Bi...   \n",
       "319  Granola - LU - 200 g e is commonly known as Bi...   \n",
       "\n",
       "                                              question  \\\n",
       "0    What is the common name of Eau de Source - Cri...   \n",
       "1    What are some ingredients in Eau de Source - C...   \n",
       "2    What is the packaging of Eau de Source - Crist...   \n",
       "3    What is the brand of Eau de Source - Cristalin...   \n",
       "4    What is the common name of Prince Chocolat bis...   \n",
       "..                                                 ...   \n",
       "315  What is the brand of Levure de bière - Gerblé ...   \n",
       "316  What is the common name of Granola - LU - 200 ...   \n",
       "317  What are some ingredients in Granola - LU - 20...   \n",
       "318   What is the packaging of Granola - LU - 200 g e?   \n",
       "319       What is the brand of Granola - LU - 200 g e?   \n",
       "\n",
       "                                                answer   id      title  \n",
       "0     {'text': ['Spring water'], 'answer_start': [56]}    1    Title 1  \n",
       "1            {'text': ['water'], 'answer_start': [63]}    2    Title 2  \n",
       "2    {'text': ['Aluminium-can, HdpeFilm-packet, PpF...    3    Title 3  \n",
       "3       {'text': ['Cristaline'], 'answer_start': [16]}    4    Title 4  \n",
       "4    {'text': ['BISCUITS FOURRÉS (35%) PARFUM CHOCO...    5    Title 5  \n",
       "..                                                 ...  ...        ...  \n",
       "315         {'text': ['Gerblé'], 'answer_start': [18]}  316  Title 316  \n",
       "316  {'text': ['Biscuits sablés nappés de chocolat ...  317  Title 317  \n",
       "317  {'text': [',wheat flour 48%, milk chocolate 27...  318  Title 318  \n",
       "318  {'text': ['fr:sachet plastique, fr:étui carton...  319  Title 319  \n",
       "319             {'text': ['LU'], 'answer_start': [10]}  320  Title 320  \n",
       "\n",
       "[320 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Let us split the dataset into training, and validation\n",
    "qac_dataset = split_dataset(qac_result, split_ratio=0.8)\n",
    "training_list = qac_dataset[\"train\"]\n",
    "validation_list = qac_dataset[\"validation\"]\n",
    "df_training = pd.DataFrame(training_list)\n",
    "df_validation = pd.DataFrame(validation_list)\n",
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1c23c1a-f658-459c-b31b-093f4a8781de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = convert_to_dataset_dict(df_training, df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75506087-0a06-45b0-974e-e4e9658e7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the tutorial: https://huggingface.co/learn/nlp-course/en/chapter7/7 we'll use the bert-base-cased model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "#Based on the tutorial we want to insert tokens create a sentence of this form: \n",
    "#[CLS] question [SEP] context [SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903764a-ca85-406c-85a4-aabbbe9efbb2",
   "metadata": {},
   "source": [
    "## Begin Testing preproccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37111e3f-2b11-4569-8629-e116f58cded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] What is the common name of Eau de Source - Cristaline - 1, 5 L? [SEP] Eau de Source - Cristaline - 1, 5 L is commonly known as Spring water. The ingredients are water. The packaging includes Aluminium - can, HdpeFilm - packet, PpFilm - wrapper, Ldpe - film. The brand is Cristaline. It falls under the categories Beverages, [SEP]\n",
      "[CLS] What is the common name of Eau de Source - Cristaline - 1, 5 L? [SEP] The packaging includes Aluminium - can, HdpeFilm - packet, PpFilm - wrapper, Ldpe - film. The brand is Cristaline. It falls under the categories Beverages, Waters, Spring waters. It has certifications like Triman. It originates from Embouteillée à 24610 Saint - [SEP]\n",
      "[CLS] What is the common name of Eau de Source - Cristaline - 1, 5 L? [SEP], Ldpe - film. The brand is Cristaline. It falls under the categories Beverages, Waters, Spring waters. It has certifications like Triman. It originates from Embouteillée à 24610 Saint - Martin de Gurson France and the origin of ingredients is France, fr : Saint - Martin de Gurson. It [SEP]\n",
      "[CLS] What is the common name of Eau de Source - Cristaline - 1, 5 L? [SEP] Spring waters. It has certifications like Triman. It originates from Embouteillée à 24610 Saint - Martin de Gurson France and the origin of ingredients is France, fr : Saint - Martin de Gurson. It is manufactured in Saint - Martin de Gurson, France, 24610. It is sold in countries like Belgium, Côte d [SEP]\n",
      "[CLS] What is the common name of Eau de Source - Cristaline - 1, 5 L? [SEP] Gurson France and the origin of ingredients is France, fr : Saint - Martin de Gurson. It is manufactured in Saint - Martin de Gurson, France, 24610. It is sold in countries like Belgium, Côte d'Ivoire, France, Germany, Guadeloupe, Italy, Luxembourg, Mali, Martinique, New Caledonia, [SEP]\n",
      "[CLS] What is the common name of Eau de Source - Cristaline - 1, 5 L? [SEP] in Saint - Martin de Gurson, France, 24610. It is sold in countries like Belgium, Côte d'Ivoire, France, Germany, Guadeloupe, Italy, Luxembourg, Mali, Martinique, New Caledonia, Switzerland, United Kingdom. The nutrition grade is Very good nutritional quality. The NOVA score is Unprocessed or [SEP]\n",
      "[CLS] What is the common name of Eau de Source - Cristaline - 1, 5 L? [SEP]voire, France, Germany, Guadeloupe, Italy, Luxembourg, Mali, Martinique, New Caledonia, Switzerland, United Kingdom. The nutrition grade is Very good nutritional quality. The NOVA score is Unprocessed or minimally processed foods. It can be found in stores such as Carrefour, Leclerc, Auchan, Intermar [SEP]\n",
      "[CLS] What is the common name of Eau de Source - Cristaline - 1, 5 L? [SEP] United Kingdom. The nutrition grade is Very good nutritional quality. The NOVA score is Unprocessed or minimally processed foods. It can be found in stores such as Carrefour, Leclerc, Auchan, Intermarché, Super U, E. Leclerc. [SEP]\n"
     ]
    }
   ],
   "source": [
    "#Test tokenizer format\n",
    "#See the format of splitting the context\n",
    "context = dataset[\"train\"][0]['context']\n",
    "question = dataset[\"train\"][0]['question']\n",
    "\n",
    "inputs = tokenizer(question, \n",
    "                   context, \n",
    "                   max_length = 100, \n",
    "                   truncation=\"only_second\", \n",
    "                   stride = 50, \n",
    "                   return_overflowing_tokens=True,)\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd9f978-0ff9-45a9-8a70-a93e884f9006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18f4b903-47ed-46be-a9f9-6449b50419cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4 examples gave 53 features.\n",
      "Here is where each comes from: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3].\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    dataset[\"train\"][2:6][\"question\"],\n",
    "    dataset[\"train\"][2:6][\"context\"],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c51aaf1-33ae-4139-8e4b-087c3ba8ce5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([51,\n",
       "  24,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  27,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  51,\n",
       "  30,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [79,\n",
       "  52,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  30,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  73,\n",
       "  52,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = dataset[\"train\"][2:6][\"answer\"]\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
    "    answer = answers[sample_idx]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label is (0, 0)\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "072ca8e8-2ba9-4805-b519-b540d96a2ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: Aluminium-can, HdpeFilm-packet, PpFilm-wrapper, Ldpe-film, labels give: Aluminium - can, HdpeFilm - packet, PpFilm - wrapper, Ldpe - film\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
    "\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77337c84-81fb-4837-951f-7fceb826ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: Aluminium-can, HdpeFilm-packet, PpFilm-wrapper, Ldpe-film, decoded example: [CLS] What is the packaging of Eau de Source - Cristaline - 1, 5 L? [SEP] and the origin of ingredients is France, fr : Saint - Martin de Gurson. It is manufactured in Saint - Martin de Gurson, France, 24610. It is sold in countries like Belgium, Côte d'Ivoire, France, Germany, Guadeloupe, Italy, Luxembourg, Mali, Martinique, New Caledonia, Switzerland, United Kingdom. [SEP]\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n",
    "print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77282c9f-8eb5-4f32-a985-849de0aae33c",
   "metadata": {},
   "source": [
    "## End Testing Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f3a0e57-d034-407f-84e3-db7b2dea7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function cited from: https://huggingface.co/learn/nlp-course/en/chapter7/7\n",
    "#Processing the training data\n",
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answer\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7a2dc0a-4db8-4b0f-8f75-60c995dbad41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████| 320/320 [00:00<00:00, 2177.75 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(320, 392)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "len(dataset[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdfaa00c-e53d-4ee5-92cd-4ac3788764a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function cited directly from:https://huggingface.co/learn/nlp-course/en/chapter7/7\n",
    "#Processing the validation Data\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e114173-cefd-40ac-9542-45c3ca9ca4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████| 80/80 [00:00<00:00, 3385.13 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80, 116)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = dataset[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    ")\n",
    "len(dataset[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b1328-e664-4aa1-b7c6-cc7c699cd9ee",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4255da3d-bc39-4d58-8adb-c9eadf598baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████| 40/40 [00:00<00:00, 3587.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "small_eval_set = dataset[\"validation\"].select(range(40))\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5714c608-5243-4829-9c6c-01bd516ee37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54407fcb-0208-4db0-9895-e42481b96e5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForQuestionAnswering\n\u001b[1;32m      4\u001b[0m eval_set_for_model \u001b[38;5;241m=\u001b[39m eval_set\u001b[38;5;241m.\u001b[39mremove_columns([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74949b9c-739b-44cb-b6f7-20c27e1e67bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
